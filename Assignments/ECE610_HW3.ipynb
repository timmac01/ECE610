{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidePrompt": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "\n",
    "from ipynb.fs.full.ECE610_HW2 import pca_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Part 1 Logistic Regression\n",
    "\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self,lr = 0.001,iterations = 1000):\n",
    "        \n",
    "        # Initializae running parameters\n",
    "        self.iterations = iterations\n",
    "        self.lr = lr\n",
    "        \n",
    "    def fit(self,data): # Data has labels in first column\n",
    "                        # Data is a pandas dataframe\n",
    "        \n",
    "        n_samples = data.shape[0] # Number of samples in dataset\n",
    "        \n",
    "        # Augment feature vector to include constant bias term\n",
    "        bias = np.ones((n_samples,1)).reshape(n_samples,1) # Add Bias\n",
    "        \n",
    "        # Add bias to matrix of \n",
    "        try:\n",
    "            data.insert(1,'bias',bias)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Convert to numpy array\n",
    "        data = data.to_numpy()\n",
    "        \n",
    "        # format to M x N matrix\n",
    "        data = data.transpose()\n",
    "\n",
    "        # Seperate Labels and Features\n",
    "        labels = data[0,:]\n",
    "        features = data[1:,:]\n",
    "        \n",
    "        # Number of features\n",
    "        n_inputs = features.shape[0]\n",
    "\n",
    "        # Init random weight vector\n",
    "        self.weights = np.random.randn(n_inputs,1).reshape(n_inputs,1)\n",
    "        \n",
    "        # Init counter for iterations \n",
    "        count = 0\n",
    "        \n",
    "        # Add epsilon for convergence \n",
    "        epsilon = np.array([self.lr/1000]*n_inputs).reshape(n_inputs,1)\n",
    "        \n",
    "        # Perform iterations\n",
    "        while count < self.iterations:\n",
    "            \n",
    "            # Predict class using current weights\n",
    "            yPred = sigmoid(np.matmul(self.weights[1:].transpose(),features[1:,:]) + self.weights[0])\n",
    "            \n",
    "            # compute gradient\n",
    "            gradient = np.matmul((yPred-labels),features.transpose())\n",
    "            gradient = gradient.reshape((n_inputs,1))\n",
    "            \n",
    "            # Store previously computed weight\n",
    "            self.weights0 = self.weights\n",
    "            \n",
    "            # Update Weights\n",
    "            self.weights = self.weights - self.lr*gradient\n",
    "\n",
    "            # Check for convergence\n",
    "            if (np.abs(self.weights - self.weights0) < epsilon).all():\n",
    "                print(\"Total iterations ran: {}\".format(count))\n",
    "                break\n",
    "            \n",
    "            count += 1\n",
    "    \n",
    "        return self.weights\n",
    "    \n",
    "    def test(self,data):\n",
    "        \n",
    "        # Convert to array of M x N with labels in first row\n",
    "        data = data.to_numpy().transpose()\n",
    "        \n",
    "        # N test samples\n",
    "        n_test = data.shape[1]\n",
    "        \n",
    "        # Seperate labels and inputs\n",
    "        target = data[0,:]\n",
    "        x = data[1:,:]\n",
    "        \n",
    "        # Predict Probability of Class 1\n",
    "        yPred = sigmoid(np.matmul(self.weights[1:].transpose(),x) + self.weights[0])\n",
    "        \n",
    "        # Assign Class label\n",
    "        classification = yPred.round()\n",
    "        \n",
    "        # Calculate classification accuracy\n",
    "        misclassified = np.abs(classification - target)\n",
    "        acc = (n_test-misclassified.sum())/n_test\n",
    "\n",
    "        print(\"Classification Accuracy: {:.2%}\".format(acc))\n",
    "        \n",
    "        return yPred\n",
    "    \n",
    "def sigmoid(a):\n",
    "    \"\"\" Sigmoid Function \"\"\"\n",
    "    return 1./(1.+np.exp(-a))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Import TwoClassTrain and TwoClassTest data\n",
    "train_df_data = pd.read_csv(\"TwoClassTrain.csv\",header = None, index_col = False)\n",
    "test_df_data = pd.read_csv(\"TwoClassTest.csv\",header = None, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "hidePrompt": true,
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total iterations ran: 2801\n[[ 1.27291967]\n [-0.60316211]\n [ 0.10441946]]\nClassification Accuracy: 79.25%\n"
    }
   ],
   "source": [
    "# Train logistic regression classifier for TwoClassTrain data\n",
    "net = logistic_regression(lr = .00005,iterations = 10000)\n",
    "weights = net.fit(train_df_data)\n",
    "print(weights)\n",
    "\n",
    "# Run Test data\n",
    "yPred = net.test(test_df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Import MNIST2Class data\n",
    "data = pd.read_csv(\"MNIST2class.csv\",header = None, index_col = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "# Set class labels to 0 and 1\n",
    "data.loc[data[0] == 4,0] = 0\n",
    "data.loc[data[0] == 8,0] = 1\n",
    "\n",
    "# N test samples\n",
    "n = data.shape[0]\n",
    "\n",
    "# Run PCA algorithm for feature generation\n",
    "pca = pca_analysis(data)\n",
    "mean_vec, w, v, sorted_index = pca.solve()\n",
    "score = pca.proj()\n",
    "score = np.real(score)\n",
    "score = score.transpose()\n",
    "data = data.to_numpy()\n",
    "\n",
    "# Seperate labels \n",
    "labels = data[:,0].reshape(n,1)\n",
    "\n",
    "# Combine Labels with 1st and 2nd Principal Component Projections\n",
    "data = np.concatenate((labels,score),axis = 1)\n",
    "\n",
    "# Seperate Training Data\n",
    "train = np.concatenate((data[data[:,0] == 0][:800,:3],data[data[:,0] == 1][:800,:3]))\n",
    "\n",
    "train_df_MNIST = pd.DataFrame(train)\n",
    "\n",
    "X_train_MNIST = train_df_MNIST.to_numpy()[:,1:]\n",
    "y_train_MNIST = train_df_MNIST.to_numpy()[:,0].ravel()\n",
    "\n",
    "# Seperate Test Data\n",
    "test = np.concatenate((data[data[:,0] == 0][800:,:3],data[data[:,0] == 1][800:,:3]))\n",
    "\n",
    "test_df_MNIST = pd.DataFrame(test)\n",
    "\n",
    "X_test_MNIST = test_df_MNIST.to_numpy()[:,1:]\n",
    "y_test_MNIST = test_df_MNIST.to_numpy()[:,0].ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[ 1716.04487538]\n [-2555.10192338]\n [-1364.7718019 ]]\nClassification Accuracy: 91.50%\n"
    }
   ],
   "source": [
    "# Train logistic regression classifier for MNIST Data\n",
    "net = logistic_regression(lr = 0.01,iterations = 10000)\n",
    "weights = net.fit(train_df_MNIST)\n",
    "print(weights)\n",
    "yPred = net.test(test_df_MNIST)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "# Seperate Training data and labels\n",
    "X_train = train_df_data[[1,2]].to_numpy()\n",
    "y_train = train_df_data[[0]].to_numpy().ravel()\n",
    "\n",
    "# Seperate Test data and labels\n",
    "X_test = test_df_data[[1,2]].to_numpy()\n",
    "y_test = test_df_data[[0]].to_numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "def test_accuracy(target, prediction): #Inputs are columns\n",
    "    \n",
    "    \"\"\"Calculate Accuracy of classifier using target labels and predictions\"\"\"\n",
    "    n_points = target.shape[0]\n",
    "    misclassified = np.abs(target - prediction)\n",
    "\n",
    "    return (n_points - misclassified.sum())/n_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "source": [
    "# Init SVM model for linear, polynomial, and radial basis function\n",
    "linear_svc = svm.SVC(kernel = 'linear')\n",
    "poly_svc = svm.SVC(kernel = 'poly')\n",
    "rbf_svc = svm.SVC(kernel = 'rbf')\n",
    "\n",
    "# Fit model to data\n",
    "linear_svc.fit(X_train,y_train)\n",
    "poly_svc.fit(X_train,y_train)\n",
    "rbf_svc.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SVM Linear Kernel Acc: 77.50%\nSVM Polynomial Kernel Acc: 99.50%\nSVM Radial Basis Function Kernel Acc: 99.50%\n"
    }
   ],
   "source": [
    "# Predict class of test data\n",
    "yPred_linear = linear_svc.predict(X_test)\n",
    "acc_linear = test_accuracy(y_test,yPred_linear)\n",
    "\n",
    "yPred_poly = poly_svc.predict(X_test)\n",
    "acc_poly = test_accuracy(y_test,yPred_poly)\n",
    "\n",
    "yPred_rbf = rbf_svc.predict(X_test)\n",
    "acc_rbf = test_accuracy(y_test,yPred_poly)\n",
    "\n",
    "print(\"SVM Linear Kernel Acc: {:.2%}\".format(acc_linear))\n",
    "print(\"SVM Polynomial Kernel Acc: {:.2%}\".format(acc_poly))\n",
    "print(\"SVM Radial Basis Function Kernel Acc: {:.2%}\".format(acc_rbf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma=0.25, kernel='rbf',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
     },
     "metadata": {},
     "execution_count": 50
    }
   ],
   "source": [
    "# MNIST Data\n",
    "\n",
    "# Init SVM model for linear, polynomial, and radial basis function\n",
    "linear_svc_MNIST = svm.SVC(kernel = 'linear')\n",
    "poly_svc_MNIST = svm.SVC(kernel = 'poly')\n",
    "rbf_svc_MNIST = svm.SVC(kernel = 'rbf',gamma = 1/4)\n",
    "\n",
    "linear_svc_MNIST.fit(X_train_MNIST,y_train_MNIST)\n",
    "poly_svc_MNIST.fit(X_train_MNIST,y_train_MNIST)\n",
    "rbf_svc_MNIST.fit(X_train_MNIST,y_train_MNIST)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SVM Linear Kernel Acc: 91.75%\nSVM Polynomial Kernel Acc: 89.25%\nSVM Radial Basis Function Kernel Acc: 89.25%\n"
    }
   ],
   "source": [
    "# Predict class of test data\n",
    "yPred_linear_MNIST = linear_svc_MNIST.predict(X_test_MNIST)\n",
    "acc_linear_MNIST = test_accuracy(y_test_MNIST,yPred_linear_MNIST)\n",
    "\n",
    "yPred_poly_MNIST = poly_svc_MNIST.predict(X_test_MNIST)\n",
    "acc_poly_MNIST = test_accuracy(y_test_MNIST,yPred_poly_MNIST)\n",
    "\n",
    "yPred_rbf_MNIST = rbf_svc_MNIST.predict(X_test_MNIST)\n",
    "acc_rbf_MNIST = test_accuracy(y_test_MNIST,yPred_poly_MNIST)\n",
    "\n",
    "print(\"SVM Linear Kernel Acc: {:.2%}\".format(acc_linear_MNIST))\n",
    "print(\"SVM Polynomial Kernel Acc: {:.2%}\".format(acc_poly_MNIST))\n",
    "print(\"SVM Radial Basis Function Kernel Acc: {:.2%}\".format(acc_rbf_MNIST))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "hidePrompt": true
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "SVM Polynomial Kernel Degree=1 Acc: 92.25%\n"
    }
   ],
   "source": [
    "# Train & Test Polynomial SVM with Degree=1\n",
    "\n",
    "poly_svc_MNIST = svm.SVC(kernel = 'poly',degree = 1)\n",
    "poly_svc_MNIST.fit(X_train_MNIST,y_train_MNIST)\n",
    "yPred_poly_MNIST = poly_svc_MNIST.predict(X_test_MNIST)\n",
    "acc_poly_MNIST = test_accuracy(y_test_MNIST,yPred_poly_MNIST)\n",
    "print(\"SVM Polynomial Kernel Degree=1 Acc: {:.2%}\".format(acc_poly_MNIST))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3.7.0 32-bit",
   "language": "python",
   "name": "python37032bitd47c87d335d14b71b3e57323c5712410"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}